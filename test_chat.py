from deepeval import assert_test
from deepeval.test_case import ConversationalTestCase, LLMTestCase
from deepeval.metrics import (
    ConversationRelevancyMetric,
    ConversationCompletenessMetric,
    RoleAdherenceMetric,
    KnowledgeRetentionMetric
)
from deepeval.models.base_model import DeepEvalBaseLLM
from config import CHAT_CONFIG, EVAL_MODEL_CONFIG, METRICS_CONFIG
from chat_client import ChatClient
import requests
import json
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import asyncio

class CustomEvalModel(DeepEvalBaseLLM):
    def __init__(self, base_url: str, api_key: str, model: str):
        super().__init__(model_name=model)
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
        self.chat_model = ChatOpenAI(
            openai_api_base=base_url,
            openai_api_key=api_key,
            model_name=model,
            temperature=0
        )
        
    def load_model(self):
        # è¿™é‡Œä¸éœ€è¦å®é™…åŠ è½½æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨APIè°ƒç”¨
        pass
        
    def _call(self, prompt: str) -> str:
        """å®ç°å®é™…çš„APIè°ƒç”¨"""
        try:
            messages = [HumanMessage(content=prompt)]
            response = self.chat_model.invoke(messages)
            return response.content
        except Exception as e:
            print(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return ""
        
    def generate(self, prompt: str) -> str:
        return self._call(prompt)
        
    async def a_generate(self, prompt: str) -> str:
        """å¼‚æ­¥ç”Ÿæˆæ–¹æ³•"""
        try:
            messages = [HumanMessage(content=prompt)]
            response = await self.chat_model.ainvoke(messages)
            return response.content
        except Exception as e:
            print(f"å¼‚æ­¥APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return ""
            
    def get_model_name(self) -> str:
        """è¿”å›æ¨¡å‹åç§°"""
        return self.model

def build_turns_from_irregular_history(messages: list) -> list[LLMTestCase]:
    """ä»ä¸è§„åˆ™çš„å†å²æ¶ˆæ¯ä¸­æ„å»ºå¯¹è¯è½®æ¬¡"""
    turns = []
    i = 0
    while i < len(messages):
        if messages[i]["role"] == "user":
            # æ‰¾æ¥ä¸‹æ¥çš„ç¬¬ä¸€ä¸ª assistant å›å¤ä½œä¸ºé…å¯¹
            j = i + 1
            while j < len(messages) and messages[j]["role"] != "assistant":
                j += 1
            if j < len(messages):  # æ‰¾åˆ°äº†é…å¯¹
                turns.append(LLMTestCase(
                    input=messages[i]["content"],
                    actual_output=messages[j]["content"]
                ))
                i = j + 1
            else:
                break  # åé¢æ²¡æœ‰ assistant å›å¤äº†
        else:
            i += 1
    return turns

def test_recent_messages(n_messages: int = 1):
    """æµ‹è¯•æœ€è¿‘çš„næ¡æ¶ˆæ¯
    
    å°†æœ€è¿‘çš„næ¡éinner_voiceæ¶ˆæ¯ä½œä¸ºä¸€ä¸ªå®Œæ•´å¯¹è¯æ¥è¯„æµ‹
    """
    # åˆå§‹åŒ–èŠå¤©å®¢æˆ·ç«¯
    client = ChatClient(
        base_url=CHAT_CONFIG["base_url"],
        token=CHAT_CONFIG["token"]
    )
    
    # åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹
    eval_model = CustomEvalModel(
        base_url=EVAL_MODEL_CONFIG["base_url"],
        api_key=EVAL_MODEL_CONFIG["api_key"],
        model=EVAL_MODEL_CONFIG["model"]
    )
    
    # è·å–æœ€è¿‘çš„æ¶ˆæ¯
    messages = client.get_recent_messages(n_messages)
    
    # æ„å»ºå¯¹è¯è½®æ¬¡
    turns = build_turns_from_irregular_history(messages)
    
    # æ„é€  ConversationalTestCase
    convo = ConversationalTestCase(
        chatbot_role=EVAL_MODEL_CONFIG["chatbot_role"],
        turns=turns
    )
    
    # ä½¿ç”¨å››ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„ä¼°
    metrics = [
        ConversationRelevancyMetric(model=eval_model),      # è¯„ä¼°å›å¤ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§
        ConversationCompletenessMetric(model=eval_model),   # è¯„ä¼°æ˜¯å¦æ»¡è¶³ç”¨æˆ·æ„å›¾
        RoleAdherenceMetric(model=eval_model),              # è¯„ä¼°æ˜¯å¦å§‹ç»ˆæ‰®æ¼”è®¾å®šè§’è‰²
        KnowledgeRetentionMetric(model=eval_model)          # è¯„ä¼°æ˜¯å¦é‡å¤è¯¢é—®å·²ç»™å‡ºä¿¡æ¯
    ]
    
    # è¿è¡Œè¯„ä¼°å¹¶æ‰“å°ç»“æœ
    print("ğŸ§ª è¯„ä¼°ç»“æœ:")
    metric_names = {
        ConversationRelevancyMetric: "å¯¹è¯ç›¸å…³æ€§",
        ConversationCompletenessMetric: "å¯¹è¯å®Œæ•´æ€§",
        RoleAdherenceMetric: "è§’è‰²ä¸€è‡´æ€§",
        KnowledgeRetentionMetric: "çŸ¥è¯†ä¿æŒ"
    }
    for metric in metrics:
        metric.measure(convo)
        metric_type = type(metric)
        metric_name = metric_names.get(metric_type, metric_type.__name__)
        print(f"{metric_name}: {metric.score:.2%}")

if __name__ == "__main__":
    test_recent_messages(20)