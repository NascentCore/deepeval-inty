from deepeval import assert_test, evaluate
from deepeval.test_case import ConversationalTestCase, LLMTestCase
from deepeval.metrics import (
    ConversationRelevancyMetric,
    ConversationCompletenessMetric,
    RoleAdherenceMetric,
    KnowledgeRetentionMetric
)
from deepeval.models.base_model import DeepEvalBaseLLM
from config import CHAT_CONFIG, EVAL_MODEL_CONFIG
from chat_client import ChatClient
import requests
import json
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import asyncio
import argparse  # å¯¼å…¥argparseåº“
from deepeval.dataset import EvaluationDataset

class CustomEvalModel(DeepEvalBaseLLM):
    def __init__(self, base_url: str, api_key: str, model: str):
        super().__init__(model_name=model)
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
        self.chat_model = ChatOpenAI(
            openai_api_base=base_url,
            openai_api_key=api_key,
            model_name=model,
            temperature=0
        )
        
    def load_model(self):
        # è¿™é‡Œä¸éœ€è¦å®é™…åŠ è½½æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨APIè°ƒç”¨
        pass
        
    def _call(self, prompt: str) -> str:
        """å®ç°å®é™…çš„APIè°ƒç”¨"""
        try:
            messages = [HumanMessage(content=prompt)]
            response = self.chat_model.invoke(messages)
            return response.content
        except Exception as e:
            print(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return ""
        
    def generate(self, prompt: str) -> str:
        return self._call(prompt)
        
    async def a_generate(self, prompt: str) -> str:
        """å¼‚æ­¥ç”Ÿæˆæ–¹æ³•"""
        try:
            messages = [HumanMessage(content=prompt)]
            response = await self.chat_model.ainvoke(messages)
            return response.content
        except Exception as e:
            print(f"å¼‚æ­¥APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return ""
            
    def get_model_name(self) -> str:
        """è¿”å›æ¨¡å‹åç§°"""
        return self.model

def build_turns_from_irregular_history(messages: list) -> list[LLMTestCase]:
    """ä»ä¸è§„åˆ™çš„å†å²æ¶ˆæ¯ä¸­æ„å»ºå¯¹è¯è½®æ¬¡"""
    turns = []
    i = 0
    while i < len(messages):
        if messages[i]["role"] == "user":
            # æ‰¾æ¥ä¸‹æ¥çš„ç¬¬ä¸€ä¸ª assistant å›å¤ä½œä¸ºé…å¯¹
            j = i + 1
            while j < len(messages) and messages[j]["role"] != "assistant":
                j += 1
            if j < len(messages):  # æ‰¾åˆ°äº†é…å¯¹
                turns.append(LLMTestCase(
                    input=messages[i]["content"],
                    actual_output=messages[j]["content"]
                ))
                i = j + 1
            else:
                break  # åé¢æ²¡æœ‰ assistant å›å¤äº†
        else:
            i += 1
    return turns

def evaluate_conversation(messages: list, source_description: str = "å¯¹è¯"):
    """è¯„ä¼°å¯¹è¯è´¨é‡
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º[{"role": "user/assistant", "content": "æ¶ˆæ¯å†…å®¹"}, ...]
        source_description: æ•°æ®æºæè¿°ï¼Œç”¨äºæ˜¾ç¤º
        
    Returns:
        dict: è¯„æµ‹ç»“æœï¼ŒåŒ…å«å››ä¸ªæŒ‡æ ‡çš„å¾—åˆ†
    """
    # åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹
    eval_model = CustomEvalModel(
        base_url=EVAL_MODEL_CONFIG["base_url"],
        api_key=EVAL_MODEL_CONFIG["api_key"],
        model=EVAL_MODEL_CONFIG["model"]
    )
    
    # æ„å»ºå¯¹è¯è½®æ¬¡
    turns = build_turns_from_irregular_history(messages)
    
    if not turns:
        print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å¯¹è¯è½®æ¬¡ï¼Œè¯·æ£€æŸ¥æ¶ˆæ¯æ ¼å¼")
        return {}
    
    # æ„é€  ConversationalTestCase
    convo = ConversationalTestCase(
        chatbot_role=EVAL_MODEL_CONFIG["chatbot_role"],
        turns=turns
    )
    
    # ä½¿ç”¨å››ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„ä¼°
    metrics = [
        ConversationRelevancyMetric(model=eval_model,verbose_mode=True),      # è¯„ä¼°å›å¤ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§
        ConversationCompletenessMetric(model=eval_model,verbose_mode=True),   # è¯„ä¼°æ˜¯å¦æ»¡è¶³ç”¨æˆ·æ„å›¾
        RoleAdherenceMetric(model=eval_model,verbose_mode=True),              # è¯„ä¼°æ˜¯å¦å§‹ç»ˆæ‰®æ¼”è®¾å®šè§’è‰²
        KnowledgeRetentionMetric(model=eval_model,verbose_mode=True)          # è¯„ä¼°æ˜¯å¦é‡å¤è¯¢é—®å·²ç»™å‡ºä¿¡æ¯
    ]
    
    # è¿è¡Œè¯„ä¼°å¹¶æ‰“å°ç»“æœ
    print(f"ğŸ§ª {source_description}çš„è¯„ä¼°ç»“æœ:")
    print(f"å…± {len(turns)} ä¸ªå¯¹è¯è½®æ¬¡")
    
    metric_names = {
        ConversationRelevancyMetric: "å¯¹è¯ç›¸å…³æ€§",
        ConversationCompletenessMetric: "å¯¹è¯å®Œæ•´æ€§",
        RoleAdherenceMetric: "è§’è‰²ä¸€è‡´æ€§",
        KnowledgeRetentionMetric: "çŸ¥è¯†ä¿æŒ"
    }
    
    datasets = EvaluationDataset(test_cases=[convo])
    results = evaluate(datasets,metrics)


    # results = {}
    # for metric in metrics:
    #     metric.measure(convo)
    #     metric_type = type(metric)
    #     metric_name = metric_names.get(metric_type, metric_type.__name__)
    #     score = metric.score
    #     results[metric_name] = score
    #     print(f"{metric_name}: {score:.2%}")
        
    return results

def test_recent_messages(n_messages: int = 1):
    """æµ‹è¯•æœ€è¿‘çš„næ¡æ¶ˆæ¯
    
    å°†æœ€è¿‘çš„næ¡éinner_voiceæ¶ˆæ¯ä½œä¸ºä¸€ä¸ªå®Œæ•´å¯¹è¯æ¥è¯„æµ‹
    """
    # åˆå§‹åŒ–èŠå¤©å®¢æˆ·ç«¯
    client = ChatClient(
        base_url=CHAT_CONFIG["base_url"],
        token=CHAT_CONFIG["token"]
    )
    
    # è·å–æœ€è¿‘çš„æ¶ˆæ¯
    messages = client.get_recent_messages(n_messages)
    
    # è¯„ä¼°å¯¹è¯
    return evaluate_conversation(messages, f"æœ€è¿‘{n_messages}æ¡æ¶ˆæ¯")

def test_from_json_file(file_path: str):
    """ä»JSONæ–‡ä»¶ä¸­è¯»å–å¯¹è¯å¹¶è¿›è¡Œè¯„æµ‹
    
    JSONæ–‡ä»¶æ ¼å¼è¦æ±‚:
    [
        {"role": "user", "content": "ç”¨æˆ·æ¶ˆæ¯1"},
        {"role": "assistant", "content": "åŠ©æ‰‹å›å¤1"},
        ...
    ]
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
    """
    # è¯»å–JSONæ–‡ä»¶
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            messages = json.load(f)
    except Exception as e:
        print(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
        return {}
    
    # è¯„ä¼°å¯¹è¯
    return evaluate_conversation(messages, f"æ–‡ä»¶ {file_path}")

if __name__ == "__main__":
    # åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨
    parser = argparse.ArgumentParser(description='å¤§æ¨¡å‹å¯¹è¯è¯„æµ‹å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')
    
    # æ·»åŠ è¯„æµ‹æœ€è¿‘æ¶ˆæ¯çš„å­å‘½ä»¤
    recent_parser = subparsers.add_parser('recent', help='è¯„æµ‹æœ€è¿‘çš„å¯¹è¯æ¶ˆæ¯')
    recent_parser.add_argument('-n', '--num', type=int, default=20, help='è¦è¯„æµ‹çš„æœ€è¿‘æ¶ˆæ¯æ•°é‡')
    
    # æ·»åŠ ä»æ–‡ä»¶è¯„æµ‹çš„å­å‘½ä»¤
    file_parser = subparsers.add_parser('file', help='ä»JSONæ–‡ä»¶è¯„æµ‹å¯¹è¯')
    file_parser.add_argument('file_path', help='å¯¹è¯JSONæ–‡ä»¶è·¯å¾„')
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œå¯¹åº”åŠŸèƒ½
    if args.command == 'recent':
        test_recent_messages(args.num)
    elif args.command == 'file':
        test_from_json_file(args.file_path)
    else:
        # é»˜è®¤è¯„æµ‹ç¤ºä¾‹æ–‡ä»¶
        test_from_json_file("conversations/sample_conversation.json")